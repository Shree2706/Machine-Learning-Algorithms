## 5. Ada boosting
AdaBoost, short for Adaptive Boosting, is a machine learning algorithm used in classification and regression tasks. 
It's a supervised learning algorithm that combines multiple weak learners into a strong learner. 

AdaBoost is a boosting technique that uses an ensemble method. It's called Adaptive Boosting because it re-assigns weights
to each instance, with higher weights assigned to incorrectly classified instances.

## 6. Gradient Boosting
Gradient boosting is a machine learning technique that combines multiple weak learners into a strong predictive model.
It's used in regression and classification tasks. 

Gradient boosting is based on a combination of boosting and gradient descent. Boosting is an ensemble method that combines
multiple weak learners (or base learners) to create a strong predictive model. Gradient descent measures the change in all
weights with regard to the change in error.

## 7. Bagging
Bagging, also known as bootstrap aggregating, is an ensemble learning technique used to improve the performance and accuracy of
machine learning algorithms. It is used to deal with bias-variance trade-offs and reduces the variance of a prediction model.

In bagging, a random sample of data in a training set is selected with replacement. This means that the individual data points
can be chosen more than once. 

Bagging is designed to improve the stability and accuracy of machine learning algorithms used in statistical classification
and regression. It also reduces variance and helps to avoid overfitting. 
